prometheus:
  enabled: true
  service:
    ipFamilyPolicy: PreferDualStack
    ipFamilies:
      - "IPv4"
      - "IPv6"
  ingress:
    enabled: true
    pathType: Prefix
    ingressClassName: "traefik"
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-production
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      hajimari.io/instance: "bloop-quarky"
      hajimari.io/enable: "true"
      hajimari.io/appName: "Prometheus"
    hosts:
      - &promhost "prometheus.${XYZ_DOMAIN}"
    tls:
      - hosts:
          - *promhost
        secretName: tls.prometheus

  thanosIngress:
    enabled: true
    pathType: Prefix
    ingressClassName: traefik
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-production
      traefik.ingress.kubernetes.io/service.serversscheme: h2c
    hosts:
      - &host "thanos-sidecar.${XYZ_DOMAIN}"
    tls:
      - hosts:
          - *host
        secretName: tls.thanos-sidecar
  thanosService:
    enabled: true
  thanosServiceMonitor:
    enabled: true

  prometheusSpec:
    resources:
      requests:
        memory: 500Mi
        cpu: 250m
      limits:
        memory: 8000Mi
    enableAdminAPI: true
    podAntiAffinity: hard
    podMonitorNamespaceSelector: {}
    podMonitorSelector: {}
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false
    replicaExternalLabelName: "replica"
    replicas: 2
    retention: 6h
    retentionSize: 8GB
    ruleSelector: {}
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorNamespaceSelector: {}
    serviceMonitorSelector: {}
    serviceMonitorSelectorNilUsesHelmValues: false
    walCompression: true

    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: "ceph-block"
          resources:
            requests:
              storage: 15Gi
    thanos:
      image: quay.io/thanos/thanos:v0.26.0
      # renovate: datasource=docker depName=quay.io/thanos/thanos
      version: v0.26.0
      objectStorageConfig:
        name: thanos-objstore-secret
        key: objstore.yml

    addtionalScrapeConfigs:
      - job_name: minio-job
        honor_timestamps: true
        metrics_path: /minio/v2/metrics/cluster
        static_configs:
          - targets:
              - "minio.storage.svc.cluster.local:9000"
      - job_name: 'blocky'
        honor_timestamps: true
        static_configs:
          - targets:
              - "blocky.networking.svc.cluster.local:4000"
      - job_name: 'uptimerobot'
        params:
          script: [uptimerobot]
        static_configs:
          - targets:
              - "uptimerobot-prometheus.monitoring.svc.cluster.local:9705"
        scrape_interval: 5m
        scrape_timeout: 90s
      - job_name: "coredns"
        honor_timestamps: true
        static_configs:
          - targets:
              - "kube-dns.kube-system:9153"
      - job_name: "jupyterhub"
        scrape_interval: 5m
        scrape_timeout: 90s
        honor_timestamps: true
        metrics_path: /metrics
        static_configs:
          - targets:
              - "proxy-api.svc.cluster.local:8001"
      - job_name: "home-assistant"
        scrape_interval: 60s
        metrics_path: /api/prometheus
        honor_timestamps: true
        # Long-Lived Access Token
        authorization:
          credentials: "${HASS_PROM_TOKEN}"
        scheme: http
        static_configs:
          - targets:
              - home-assistant.home.svc.cluster.local:8123
            labels:
              app.kubernetes.io/name: home-assistant
